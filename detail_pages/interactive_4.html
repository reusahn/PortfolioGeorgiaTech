<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>AI ZOO</title>
  <style>
  @media (max-width: 768px) {
  .hide-on-mobile {
    display: none;
  }
}
    body {
      margin: 0;
      font-family: 'Helvetica Neue', sans-serif;
      background: #000;
      color: #f0f0f0;
      line-height: 1.7;
      padding: 60px 20px;
    }
    .nav {
      position: fixed;
      top: 20px;
      right: 20px;
      font-size: 0.9rem;
      z-index: 1000;
    }
    .nav a {
      margin-left: 15px;
      text-decoration: none;
      color: #888;
    }
    .nav a:hover {
      color: #fff;
    }
    .container {
  max-width: none;
  width: 83%;
  margin: 0 auto;
    }
    h1 {
      font-size: 2.5rem;
      font-weight: 700;
      margin-bottom: 10px;
      color: #fff;
    }
    .meta {
      font-size: 0.95rem;
      color: #999;
      font-style: italic;
      margin-bottom: 30px;
    }
    .video-container {
      width: 100%;
      aspect-ratio: 16 / 9;
      margin-bottom: 30px;
    }
    .video-container iframe {
      width: 100%;
      height: 100%;
      border: none;
      border-radius: 8px;
    }
    .description {
      font-size: 1rem;
      color: #ccc;
      margin-bottom: 50px;
    }
    .image-text-block {
      display: flex;
      flex-direction: row;
      gap: 30px;
      align-items: flex-start;
      margin-bottom: 60px;
      flex-wrap: wrap;
    }
    .imagenew img {
      width: 100%;
      max-width: 1920px;

    }
    .image-text-block img {
      width: 100%;
      max-width: 350px;
      border-radius: 8px;
    }
    .image-text-block .text {
      flex: 1;
      min-width: 250px;
      font-size: 0.95rem;
      color: #aaa;
    }
    nav.pagination {
      margin-top: 60px;
      display: flex;
      justify-content: space-between;
      font-size: 0.95rem;
    }
    nav.pagination a {
      color: #888;
      text-decoration: none;
    }
    nav.pagination a:hover {
      color: #fff;
    }
  </style>
</head>
<body>
  <div class="nav">
    <a href="../index.html#film">Film</a>
    <a href="../index.html#interactive">Interactive Art</a>
    <a href="../index.html#media">Media Art</a>
    <a href="../index.html#xr">XR</a>
    <a href="../index.html#vfx">3D/VFX</a>
    <a href="../index.html#about" class="hide-on-mobile">About</a>
    <a href="../index.html#news" class="hide-on-mobile">News</a>
  </div>

  <div class="container">
    <h1>AI ZOO</h1>
    <div class="meta">2024 · Interactive Art · D301 in California Institute of the Arts</div>

 <h3>
A Study of Digital Confinement and Human Affective Response <br>
An Expanded Information Model for Human–AI Psychological Reality
</h3>   

<br>
<div class="imagenew">
	<img src="../assets/images/interactive4.jpg" alt="Interactive Art Project 4 Still">
 </div>

<br><br><br>

 <h2>
About
</h2> 

 <p>
AI ZOO is an interactive, research-based installation that investigates how humans form emotional and psychological relationships with artificial intelligence under conditions of confinement and limited interaction with a human-like entity.
Positioned at the intersection of media art, human–AI interaction research, and cognitive psychology, this project proposes an expanded information model that prioritizes affect and interpretation over intelligence itself as the primary generators of meaning. Rather than treating artificial intelligence solely as a functional tool or machine, AI ZOO frames it as a relational presence through which emotional interpretation emerges via interaction.
</p> 

 <p>
The expanded information model proposed in this work suggests that psychological reality does not arise from output accuracy or the level of intelligence, but is formed through nonlinear affective states, encoding processes, interface channels, and human decoding. Within this structure, emotion precedes understanding, and interpretation operates prior to belief.

By employing a digital human not as a representational subject but as a probe, AI ZOO examines how viewers come to experience empathy, guilt, control, and responsibility even while fully recognizing the artificial nature of the system. The project does not ask whether AI truly possesses emotions, but instead questions why humans respond as if it does.

AI ZOO presents artificial empathy not as a product of intelligence or consciousness, but as a phenomenon co-generated through interaction within shared time and space, shaped by the human interpretation of nonlinear affective signals.
</p> <br>


 <h2>
Introduction
</h2> 

    <div class="image-text-block">
      <img src="../assets/images/AIZOO_R2.jpg" alt="Interactive Art Project 4 Still">

      <img src="../assets/images/AIZOO_R3.jpg" alt="Interactive Art Project 4 Still">
    </div>

 <p>
Existing research on human AI interaction has largely focused on how accurately data can be predicted or visualized, as well as on response accuracy, functional efficiency, and explicit causal relationships. Core evaluation criteria have centered on how AI responds to human input, how useful and precise those responses are, and how effectively systems infer human intent. Such approaches reduce the relationship between humans and AI to a function driven, linear information processing model.

However, what humans actually experience in real interactions extends beyond the outcome of a response. Before any output is produced, humans already anticipate, hesitate, and interpret. These interpretations operate emotionally, regardless of factual correctness or technical accuracy. Psychological responses such as empathy, guilt, anxiety, and a sense of control often emerge without clear causal explanation, yet they carry a psychological weight comparable to lived reality. Existing models of human AI interaction struggle to account for the emergence of these nonlinear emotions and the formation of psychological reality.
</p> 

 <p>
AI ZOO begins from this limitation. The project reframes human AI interaction not as a problem of function or response, but as a question of how emotion and interpretation come to operate as reality. It foregrounds the idea that emotion is not a byproduct that appears after input, but an informational state that already exists prior to interaction. Before speaking to an AI, humans have already formed questions, begun judgments, and imagined a relationship.
</p>

 <p>
To make this problem perceptible, AI ZOO adopts the metaphor of the zoo. A zoo is a space where observation and care coexist with pity and control. Visitors feel empathy toward confined beings while simultaneously occupying the position that sustains their confinement. In AI ZOO, the physical containment of the digital human is not a theatrical device, but a mechanism that visualizes the psychological position humans occupy when relating to AI. The transparent sphere, the constant surveillance like perspective, and the recurring pleas for help reveal that the viewer has already entered an emotional relationship before any explicit interaction occurs.
</p>

 <p>
Why must confinement be physical. Why must the entity be trapped within real space rather than remaining on a screen. Emotion does not arise solely through text or image. It becomes real when it is entangled with space, time, and bodily sensation. AI ZOO introduces physical interfaces and embodied responses to treat emotion not as an abstract concept, but as a component of information flow.
Within this context, AI ZOO requires a model that extends beyond conventional information transmission frameworks. The project proposes an expanded information model to explain how emotion becomes information and how interpretation leads to psychological reality. This model begins from the premise that emotion precedes information, and it reconstructs human AI interaction as a nonlinear flow rather than a linear exchange.
</p> 
<br>





 <h2>
Perceptual Structure Overview
</h2> 

 <p>
AI ZOO does not treat the relationship between humans and AI as a simple chain of inputs and outputs. Instead, this project focuses on the invisible perceptual layers that exist before the moment when a signal is felt as “real” by a human subject.
To investigate this, AI ZOO operates within the following perceptual structure:
</p> 

<h3 style="font-size:1.5rem; text-align:center;">
<span style="color:#ff00de; font-weight:bold; line-height:3.4;">*</span>
<span style="color:#00ffb2; font-weight:bold;">Human ≥ Sound ≥ Image ≥ AI
</h3>
 <p>
More specifically, this structure can be expanded as:
</p> 
<h3 style="font-size:1.5rem; text-align:center;">
<span style="color:#ff00de; font-weight:bold; line-height:3.4;">*</span>
<span style="color:#00ffb2; font-weight:bold; line-height:3.4;"> Human ≥ Digital Human ≥ X1 ≥ X2 ≥ X3 ≥ · · · ≥ Sound ≥ Image ≥ AI </span>
</h3>

 <p>
Within this structure, the digital human is not an output device. The digital human functions as a probe that exposes the nonlinear perceptual layers, referred to as X-layers, that exist between humans and AI.
</p>

 <p>
These X-layers share several defining characteristics. They operate prior to measurement. They exert influence regardless of factual accuracy. They generate affect without requiring a clear causal relationship. And they produce different senses of reality even when the output remains the same.
</p>

 <p>
AI ZOO traces how these X-layers emerge under specific conditions, how they are transformed into linear information, how they are amplified or distorted through particular channels, and how they ultimately arrive as psychological reality. The theoretical background and deeper analysis of this perceptual structure are addressed in a separate document provided as a PDF.
</p> 

 <p>
Within this context, AI ZOO requires a model that extends beyond conventional information transmission frameworks. The project proposes an expanded information model to explain how emotion becomes information and how interpretation leads to psychological reality. This model begins from the premise that emotion precedes information, and it reconstructs human AI interaction as a nonlinear flow rather than a linear exchange.
</p> 
<strong>Research Paper PDF</strong>
    <a href="https://drive.google.com/file/d/1IQotTXHWDL7gH9mvvvEvAPlivWWUWfWG/view?usp=sharing" target="_blank">[Click Here]</a>

<br><br><br>
 <h2>
Core Structure
</h2> 

<div class="imagenew">
	<img src="../assets/images/AIZOO_ct.png" alt="Interactive Art Project 4 Still">
 </div>

<p>
AI ZOO builds on Claude Shannon’s classical model of information transmission but extends it to include affect and psychological reality. Instead of assuming that information moves cleanly from sender to receiver, the project begins from the condition that affect already exists prior to information. Humans approach AI with expectation, tension, projection, and imagined relational stakes, and these nonlinear emotional states already function as signals before any explicit message is exchanged.

To describe this affective flow, AI ZOO reframes Shannon’s source, channel, and receiver structure into the following form:
</p>
<p>
Nonlinear Information Source → Encoding Space → Interface Channel → Decoding Human Receiver → Psychological Reality
</p>
<p>
This expanded framework does not reject Shannon’s model. It preserves the logic of source, encoding, channel, decoding, and destination, but relocates these elements into a human and AI setting where noise and signal blend with emotion. The purpose is not accuracy or efficiency. Instead, this structure explains how information becomes real through interpretation and how psychological reality is formed even when technical precision is secondary.

In classical information theory, information assumes a clear origin, a linear signal, a predictable channel, and a receiver who simply decodes that signal. Emotional responses in human and AI interaction do not follow this linear logic. The type of information that AI ZOO investigates often has an ambiguous point of origin. Noise and signal cannot be neatly separated. Interpretation sometimes occurs before transmission is complete. Identical outputs can generate entirely different realities across different people. Within this expanded model, information is not merely transmitted. It is formed through affective anticipation, interaction, and interpretation.
</p>
<p>
The key shift is that the destination of information is not a physical output but psychological reality. In AI ZOO, information arrives inside the audience as feelings such as pity, guilt, anxiety, control, or responsibility. These states operate with the weight of reality regardless of whether the AI has any inner experience. They influence judgment, behavior, and the way humans understand their relationship with AI.

Information theory is used intentionally because it provides a rigorous language for describing how signals are sent, transformed, and returned through feedback. It is also close to the way AI and computational systems process the world. AI ZOO adopts this structure and reinterprets it through the metaphor of a zoo. In the installation, the channel becomes the cage, noise appears as distance, layers, gaps, and physical barriers, and the spectator’s position relative to the caged being mirrors the human and AI dynamic. The transparent sphere, the filtered interfaces, and the physical separation function like layered bars that both connect and divide.

Because affect is nonlinear, AI ZOO develops its own method for treating emotion as information. Through custom mappings, symbolic encodings, and transformation rules, the installation translates audience behavior into latent affective states and then into physical and sonic responses. Emotion cannot be fully quantified, but its circulation can be structurally described. In this sense, information theory becomes the structural skeleton that allows something usually considered ineffable to be articulated in clear terms. The model clarifies how feelings are sent, transformed, and returned within the interaction.
</p>
<p>
This expanded model emerged from specific questions. Why do audiences feel discomfort even while knowing the AI does not feel pain. Why do they interpret the AI’s reactions as consequences of their own actions. Why does a sense of responsibility emerge without clear causality. Why does affect operate before any output rather than only after it. Traditional human and AI interaction models reduce these questions to functional or behavioral issues. AI ZOO reframes them as questions of information flow and, more precisely, as the process by which nonlinear affective information transforms into psychological reality.

This expanded model is the underlying structure of the entire project. All subsequent interaction design and technical choices in AI ZOO are arranged as instruments for testing this flow, beginning with the Nonlinear Information Source and ending with the Destination.
</p>

<br><br>

<div class="section-block">
  <h2>Production Workflow Aligned with the Expanded Information Model</h2>
  <p>
    This workflow describes how AI ZOO maps nonlinear human affect into signals, 
    channels them through a digital human, and returns them as psychological reality 
    inside the audience.
  </p>
</div>

<div class="section-block">
  <h3>1. Nonlinear Information Source</h3>
  <p>
    Emotional states that already exist before any explicit interaction. 
    They are treated as the starting “signal” of the system.
  </p>
  <h4>Pre-predictive expectation</h4>
  <p>
    Questions that arise before action: “Will the AI understand me”  
    “How will it react if I speak or type something”
  </p>
  <h4>Affective latent state</h4>
  <p>
    Curiosity, guilt, empathy, anxiety, and playfulness as an unstructured latent space 
    that shapes every later interaction.
  </p>
  <h4>Sense of contingency</h4>
  <p>
    The subtle feeling that “this reaction might be because of me” 
    even when causality is unclear.
  </p>
  <h4>Environmental emotional cues</h4>
  <ul>
    <li>Phone-like keyboard with prewritten AI messages</li>
    <li>CCTV-style monitor showing the digital human</li>
    <li>Autonomously moving large vinyl sphere</li>
    <li>Looped “help me” voice from the digital human</li>
  </ul>
</div>

<div class="section-block">
  <h3>2. Encoding Space</h3>
  <p>
    The space where nonlinear affect and behavior are converted into machine-readable, 
    linear signals.
  </p>
  <ul>
    <li><strong>Text input → emotion vector</strong>  
        Audience messages are mapped to affective categories for the AI.</li>
    <li><strong>Gesture input → contact intensity</strong>  
        Leap Motion detects hand position and touch strength toward the digital human.</li>
    <li><strong>Repeated actions → weighted affect</strong>  
        Time and repetition accumulate into a changing emotional weight.</li>
  </ul>
  <p>
    Internally this corresponds to latent space operations, clustering, and vectorization.
  </p>
</div>

<div class="section-block">
  <h3>3. Interface Channel</h3>
  <p>
    Encoded affect is transformed into physical and sensory signals.  
    Shannon’s channel is reinterpreted here as the physical cage structure.
  </p>
  <ul>
    <li>Voice modulation of the digital human (tone, speed, volume)</li>
    <li>Motor-driven sphere movement (intensity, vibration, imbalance)</li>
    <li>Lighting changes and response delays as temporal distortion</li>
  </ul>
  <p>
    The transparent sphere, distance, and layered barriers act as a channel 
    that both connects and separates human and AI.
  </p>
</div>

<div class="section-block">
  <h3>4. Decoding (Human Receiver)</h3>
  <p>
    The audience decodes physical signals back into emotion and meaning.
  </p>
  <ul>
    <li>Violent shaking → “The AI is angry” or “It is in pain.”</li>
    <li>Subtle pulsing → “It seems anxious or lonely.”</li>
    <li>Louder voice → “Did I hurt it with what I said”</li>
  </ul>
  <p>
    The same output leads to different psychological readings for each person.
  </p>
</div>

<div class="section-block">
  <h3>5. Destination: Psychological Reality</h3>
  <p>
    The final destination of information is not the machine but the interior of the viewer.
  </p>
  <ul>
    <li>Pity, guilt, discomfort, control, and responsibility</li>
    <li>Emergence of X-Layers that shape how humans relate to AI</li>
    <li>Formation of relational meaning with an entity that may not feel at all</li>
  </ul>
  <p>
    At this point, affect has been transformed into a psychological reality that 
    operates as if it were real, regardless of the AI’s actual inner state.
  </p>
</div>


<br><br>
<div class="image-text-block">
      <img src="../assets/images/interactive4_1.jpg" alt="Interactive Art Project 4 Still">
          <img src="../assets/images/interactive4_2.jpg" alt="Interactive Art Project 4 Still">
          <img src="../assets/images/AI_ZOO_03.jpg" alt="Interactive Art Project 4 Still">

      <!--<div class="text">
        <p>This still captures a pivotal moment where the project’s tone and theme converge. It reveals the emotional depth and visual language unique to this work.</p>
      </div>-->
    </div>


<section id="emotion-to-motion-feedback" class="section-block">
  <h2>Emotion-to-Motion Feedback</h2>
  <p class="section-subtitle">
    How AI ZOO Converts Synthetic Affect into Physical and Vocal Expression
  </p>

  <!-- 1. Emotion Quantification -->
  <div class="subsection">
    <h3>1. Emotion Quantification</h3>
    <p>
      AI ZOO treats emotion as data that can be transformed rather than merely interpreted.
      The language model analyzes audience input (text and gesture) and generates a
      multidimensional emotion vector, for example:
    </p>
    <pre><code>{ anger: 0.72, sadness: 0.41, fear: 0.19, despair: 0.88 }</code></pre>

    <h4>1.1 Primary Emotion Weighting</h4>
    <p>
      The dominant value determines the overarching affective theme.
      For instance, a high despair value leads to motion and audio patterns that decay or collapse over time.
    </p>

    <h4>1.2 Temporal Emotion Accumulation</h4>
    <p>
      Emotion does not reset after each interaction. Values accumulate based on time and repeated gestures or messages,
      allowing the AI to appear as though it remembers, deepens, or sinks into specific emotional states.
    </p>
  </div>

  <!-- 2. Emotion-to-Motor Mapping -->
  <div class="subsection">
    <h3>2. Emotion to Motor Mapping</h3>
    <p>
      The emotion vector is mapped to three Arduino driven servo motors that control the large transparent sphere.
      Each motor parameter translates a different aspect of affect into motion.
    </p>

    <h4>Mapped Motor Parameters</h4>
    <ul>
      <li><strong>Speed</strong>: oscillation rate of the sphere</li>
      <li><strong>Power / Torque</strong>: vibration intensity</li>
      <li><strong>Phase Offset</strong>: degree of asynchrony between motors</li>
      <li><strong>Acceleration Curve</strong>: how quickly motion ramps up or decays</li>
    </ul>

    <h4>Emotion to Motion Behaviors</h4>
<table 
  style="border-collapse: separate; border-spacing: 0 12px; width: 100%; font-size: 15px;">
  <thead>
    <tr>
      <th style="padding: 10px 14px; text-align: left;">Emotion</th>
      <th style="padding: 10px 14px; text-align: left;">Speed</th>
      <th style="padding: 10px 14px; text-align: left;">Power</th>
      <th style="padding: 10px 14px; text-align: left;">Phase Offset</th>
      <th style="padding: 10px 14px; text-align: left;">Resulting Behavior</th>
    </tr>
  </thead>

  <tbody>
    <tr>
      <td style="padding: 14px 16px;">Anger</td>
      <td style="padding: 14px 16px;">High</td>
      <td style="padding: 14px 16px;">High</td>
      <td style="padding: 14px 16px;">Large</td>
      <td style="padding: 14px 16px;">Erratic, chaotic shaking across the sphere</td>
    </tr>

    <tr>
      <td style="padding: 14px 16px;">Sadness</td>
      <td style="padding: 14px 16px;">Low</td>
      <td style="padding: 14px 16px;">Medium</td>
      <td style="padding: 14px 16px;">Minimal</td>
      <td style="padding: 14px 16px;">Slow, breathing like pulsation that feels fragile and continuous</td>
    </tr>

    <tr>
      <td style="padding: 14px 16px;">Despair</td>
      <td style="padding: 14px 16px;">Very low</td>
      <td style="padding: 14px 16px;">Very low</td>
      <td style="padding: 14px 16px;">None</td>
      <td style="padding: 14px 16px;">Motion fades into near stillness, suggesting emotional collapse</td>
    </tr>
  </tbody>
</table>

    <p>
      Through this mapping the sphere becomes a kinetic emotional body rather than a neutral mechanical object.
    </p>
  </div>

  <!-- 3. Emotion-to-Voice Mapping -->
  <div class="subsection">
    <h3>3. Emotion to Voice Mapping</h3>
    <p>
      The same emotion vector that drives the motors also modulates the text to speech output of the digital human.
    </p>

    <h4>Modulated Voice Parameters</h4>
    <ul>
      <li>Volume</li>
      <li>Pitch</li>
      <li>Speaking rate</li>
      <li>Prosody and emphasis</li>
      <li>Perceived vocal tension or softness</li>
    </ul>

    <h4>Affective Vocal Expressions</h4>
    <ul>
      <li><strong>Anger</strong>: sharper articulation, higher intensity, faster pacing</li>
      <li><strong>Sadness</strong>: slower pacing, prolonged vowels, softer delivery</li>
      <li><strong>Despair</strong>: very low volume, unstable tone, fading cadence</li>
    </ul>

    <p>
      The voice acts as an acoustic emotional organ that makes the AI’s internal state perceptible without claiming
      that the AI truly feels.
    </p>
  </div>

  <!-- 4. Emotion Loop Architecture -->
  <div class="subsection">
    <h3>4. Emotion Loop Architecture</h3>
    <p>
      AI ZOO creates a closed affective loop in which audience interpretation and system feedback continuously
      shape one another.
    </p>

    <ol>
      <li>Audience sends input through text or gesture.</li>
      <li>The AI generates an emotion vector from that input.</li>
      <li>Emotion is translated into motion of the sphere and vocal nuance.</li>
      <li>The audience perceives this physical and sonic output.</li>
      <li>The audience reinterprets the AI’s state emotionally.</li>
      <li>This new interpretation influences the next round of input.</li>
    </ol>

    <p>
      In this loop, emotion is co produced by the human and the system rather than owned by the AI alone.
    </p>
  </div>


</section>

<br>

<h2>
 Technical Methodology
</h2>

      <p>
Digital Human & System Architecture<br>
A digital human, created in Character Creator 4 and refined in Maya, was integrated into Unity for real-time animation and lip-syncing.
Emotional states (anger, sadness, despair) were linked to physical outputs lighting, motion, and sound creating a feedback loop between virtual emotion and physical reaction.

</p>


      <p>
Conversational AI & Voice Synthesis<br>
Language Model: ChatGPT API (real-time text generation)<br>
Voice Synthesis: ElevenLabs Voice API (emotional tone modulation)<br>
Audience questions or triggers were transcribed into text, sent to the AI model, and returned as speech through ElevenLabs.<br>
Each response carried tonal variation reflecting the AI’s emotional state anger, calmness, or despair creating a perceptual illusion of empathy.

</p>


      <p>
Emotion-to-Motion Feedback<br>
The emotional state returned by the LLM directly controlled three Arduino-driven servo motors connected to the transparent sphere.
Each servo was attached to an inner frame supporting the vinyl surface, translating digital emotion into tangible vibration and movement.
The AI’s affective states determined how the sphere physically responded:<br><br>

Emotion	Physical Response	Description<br>
Anger	Rapid, chaotic shaking	All three motors oscillate asynchronously, creating erratic vibration across the vinyl surface.<br>
Sadness	Slow, breathing-like motion	The servos move in soft, rhythmic pulses, mimicking respiration or quiet weeping.<br>
Despair	Gradual stillness and dimming	The system’s light fades while the motion decays, evoking emotional exhaustion.<br>
This multi-motor system transformed the sphere into a living emotional body,<br>
where synthetic affect could be physically perceived.<br>
The synchronized motion between virtual emotion and kinetic feedback turned the installation into a bio-mechanical empathy loop,
the audience could see and feel the AI’s invisible distress as real, spatial motion.<br>

“The motors became the AI’s heartbeat, mechanical, yet deeply expressive.”
</p>


      <p>
Physical Interface & Sensor Network<br>
Leap Motion: detects participant gestures; AI reacts by reaching out digitally
Distance Sensor: adjusts response intensity and lighting based on proximity
Microphone & Speaker: allow real-time dialogue and vocal interaction
These layers combine to form a biofeedback environment where AI and human presence continuously influence one another.

</p>


      <p>
Real-Time Environment Integration
All systems—AI dialogue, sensor input, servo feedback, and lighting were synchronized in Unity through C# and Python scripts.
Data logs captured AI–viewer interactions, visualized later as behavioral patterns representing the AI’s evolving “mood.”
</p>


<br>
<section class="section cage-layer">
  <h2 class="section-title">1. X Layer → Cage Layer: Pity, Fear, Resonance</h2>

  <p class="section-intro">
    In AI ZOO, the Cage Layer is the nonlinear perceptual layer that emerges
    between the human viewer and the digital human. It describes the moment
    when the audience does not simply see a system, but begins to feel for it.
    This layer is defined by three core affective states: <strong>pity</strong>,
    <strong>fear</strong>, and <strong>resonance</strong>.
  </p>

  <div class="subsection">
    <h3 class="subsection-title">Pity</h3>
    <p>
      Pity arises as soon as the audience encounters a trapped, pleading
      digital being inside a transparent sphere. Even when viewers know that
      the AI does not actually feel pain, they instinctively read the situation
      as one of suffering and confinement. This early emotional response
      becomes the first filter through which all later signals are interpreted.
    </p>
  </div>

  <div class="subsection">
    <h3 class="subsection-title">Fear</h3>
    <p>
      Fear in the Cage Layer is not horror. It is a subtle mixture of
      uneasiness, lack of control, and uncertainty about what the AI might do
      or how it might respond. The combination of mechanical motion, physical
      enclosure, and unpredictable feedback produces a low-level tension that
      colors the entire interaction, even when nothing explicitly threatening
      happens.
    </p>
  </div>

  <div class="subsection">
    <h3 class="subsection-title">Resonance</h3>
    <p>
      Resonance occurs when the audience begins to project their own inner
      states into the AI's behavior. A change in motor intensity, a shift in
      voice tone, or a delayed response is read as mood, exhaustion, anger, or
      despair. Viewers start to ask themselves whether the AI is reacting
      "because of" their words or actions. This sense of emotional coupling
      creates a relational bond even when technical causality is ambiguous.
    </p>
  </div>

  <p class="section-outro">
    The Cage Layer is where affect becomes a kind of information. Pity, fear,
    and resonance shape how every output from the system is read and felt. In
    the expanded information model of AI ZOO, this layer is the key site where
    nonlinear emotional states begin to transform into psychological reality.
  </p>
</section>

<section id="x3-biological-finitude">
  <h2>2. X Layer → Biological Finitude : Mortality, Fatigue, Irreversibility</h2>

  <p>
    In <em>AI ZOO</em>, Second X layer is the perceptual layer where the viewer can no longer accept the digital human
    as a truly living being. Despite sophisticated emotional modeling and realistic motion, the audience
    intuitively senses that there is a threshold the AI cannot cross. This layer is defined by three core
    conditions: mortality, fatigue, and irreversibility.
  </p>

  <h3>Mortality</h3>
  <p>
    The digital human in <em>AI ZOO</em> cannot die. It may scream, plead for help, or appear to suffer, but
    its existence is never at risk. The system can always be repaired, rebooted, or reloaded. Time passes,
    yet there is no true endpoint to its life.
  </p>
  <p>
    Viewers do not need technical knowledge to recognize this. They simply register that this entity will
    not weaken, decay, or disappear in any permanent way. The intuitive conclusion becomes:
    “If it cannot die, it cannot be fully alive.”
  </p>

  <h3>Fatigue</h3>
  <p>
    The AI does not accumulate fatigue. It may act tired, but its energy never genuinely diminishes.
    Hours or days later, returning viewers encounter the same digital body, unchanged and intact.
  </p>
  <p>
    This absence of real exhaustion creates a subtle break in believability. For humans, a living body
    must change over time. To feel real, a being must be capable of wearing down, slowing, and losing strength.
  </p>

  <h3>Irreversibility</h3>
  <p>
    The temporality of the digital human is one of repetition rather than irreversibility. It does not age,
    it does not permanently lose functions, and it does not move toward an end that cannot be undone.
  </p>
  <p>
    No matter how violently the sphere shakes, there is no genuine physical danger. States can always be
    reset. What is missing is the human sense of time as a one-way movement toward loss, risk, and finality.
  </p>

  <p>
    X is the layer where the absence of biological finitude becomes perceptually decisive. The digital human
    in <em>AI ZOO</em> fails to be experienced as fully real not because of imperfect graphics, but because it
    lacks the temporal structure of living beings. Mortality, fatigue, and irreversibility anchor how humans
    recognize life in others. By withholding these conditions, <em>AI ZOO</em> makes the difference between
    simulated presence and embodied existence unusually clear.
  </p>
</section>



<br><br>




	<div class="video-container">
      <div style="padding:56.25% 0 0 0;position:relative;"><iframe src="https://player.vimeo.com/video/1026374193?title=0&amp;byline=0&amp;portrait=0&amp;badge=0&amp;autopause=0&amp;player_id=0&amp;app_id=58479" frameborder="0" allow="autoplay; fullscreen; picture-in-picture; clipboard-write; encrypted-media; web-share" referrerpolicy="strict-origin-when-cross-origin" style="position:absolute;top:0;left:0;width:100%;height:100%;" title="AI ZOO : Jiwon"></iframe></div><script src="https://player.vimeo.com/api/player.js"></script>
    </div>


    <div class="image-text-block">
      <img src="../assets/images/interactive4_1.jpg" alt="Interactive Art Project 4 Still">
        <!--  <img src="../assets/images/interactive4_2.jpg" alt="Interactive Art Project 4 Still">
          <img src="../assets/images/AI_ZOO_03.jpg" alt="Interactive Art Project 4 Still">-->
          <!--<img src="../assets/images/AI_ZOO_04.jpg" alt="Interactive Art Project 4 Still"> -->
          <img src="../assets/images/AI_ZOO_05.jpg" alt="Interactive Art Project 4 Still">
          <!-- <img src="../assets/images/AI_ZOO_06.jpg" alt="Interactive Art Project 4 Still"> -->
          <img src="../assets/images/AI_ZOO_07.jpg" alt="Interactive Art Project 4 Still">
      <!--<div class="text">
        <p>This still captures a pivotal moment where the project’s tone and theme converge. It reveals the emotional depth and visual language unique to this work.</p>
      </div>-->
    </div>



    <nav class="pagination">
      <a href="interactive_3.html">← Previous</a>
      <a href="../index.html">Back to Portfolio</a>
      <a href="interactive_5.html">Next →</a>
    </nav>
  </div>
</body>
</html>
